{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "# 의사록(MPB)\n",
    "\n",
    "import requests, os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 각자 파일 경로\n",
    "my_local_pdf_dir_path = 'secret_MPB_minutes'\n",
    "# 디렉토리 생성\n",
    "os.makedirs(f'./{my_local_pdf_dir_path}', exist_ok=True)\n",
    "\n",
    "for i in range(1, 40):\n",
    "    b_url = \"https://www.bok.or.kr/portal/bbs/B0000245/list.do?menuNo=200761\"\n",
    "    params = {\n",
    "        'pageIndex' : i\n",
    "    }\n",
    "\n",
    "    response = requests.get(b_url, params=params)\n",
    "    soup = BeautifulSoup(response.content)\n",
    "\n",
    "    li_list = []\n",
    "    li = soup.find('div', class_='bdLine type2').find('ul').find('li')\n",
    "    li_li = li.find_next_siblings('li', li)\n",
    "    li_list.append(li)\n",
    "    li_list.extend(li_li)\n",
    "\n",
    "    print(i)\n",
    "\n",
    "    for x in range(0, len(li_list)):\n",
    "        try:\n",
    "            link = li_list[x].find('div', class_='fileGoupBox').find('ul').find('li').find('a').attrs['href']\n",
    "            name_ = li_list[x].find('div', class_='fileGoupBox').find('ul').find('li').find('a').getText().strip()\n",
    "            title = li_list[x].find('div', class_='row').find('span').find('a').find('span').find('span').text\n",
    "            url2 = 'http://www.bok.or.kr' + link\n",
    "            file_res = requests.get(url2)\n",
    "            if name_[-4:]=='.hwp':\n",
    "                link = li_list[x].find('div', class_='fileGoupBox').find('ul').find_all('li')[1].find('a').attrs['href']\n",
    "                url2 = 'http://www.bok.or.kr' + link\n",
    "                file_res = requests.get(url2)\n",
    "                with open('./{}/{}.pdf'.format(my_local_pdf_dir_path, title), 'wb') as f:\n",
    "                    f.write(file_res.content)\n",
    "            else:\n",
    "                with open('./{}/{}.pdf'.format(my_local_pdf_dir_path, title), 'wb') as f:\n",
    "                    f.write(file_res.content)\n",
    "\n",
    "        except:\n",
    "            try:\n",
    "                link2 = li_list[x].find('div').find('div').find('a').attrs['href']\n",
    "                name_ = li_list[x].find('div', class_='fileGoupBox').find('ul').find('li').find('a').getText().strip()\n",
    "                title = li_list[x].find('div', class_='row').find('span').find('a').find('span').find('span').text\n",
    "                url3 = 'http://www.bok.or.kr' + link2\n",
    "                file_res2 = requests.get(url3)\n",
    "                if name_[-4:]=='.hwp':\n",
    "                    link2 = li_list[x].find('div').find('div').find_all('a')[1].attrs['href']\n",
    "                    url3 = 'http://www.bok.or.kr' + link2\n",
    "                    with open('./{}/{}.pdf'.format(my_local_pdf_dir_path, title), 'wb') as f:\n",
    "                        f.write(file_res.content)\n",
    "                else:\n",
    "                    with open('./{}/{}.pdf'.format(my_local_pdf_dir_path, title), 'wb') as f:\n",
    "                        f.write(file_res.content)\n",
    "            except:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPB minutes pdf download code\n",
    "import requests, urllib, os\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# 각자 파일 경로\n",
    "my_local_pdf_dir_path = 'secret_MPB_minutes'\n",
    "# 디렉토리 생성\n",
    "os.makedirs(f'./{my_local_pdf_dir_path}', exist_ok=True)\n",
    "# b_url: base url, {page}: page number\n",
    "b_url = \"http://www.bok.or.kr/portal/bbs/B0000245/list.do?menuNo=200761&pageIndex={page}\"\n",
    "\n",
    "for i in tqdm(range(1, 40)):\n",
    "    response = requests.get(b_url.format(i))\n",
    "    soup = BeautifulSoup(response.content)\n",
    "\n",
    "    # file_name_list: 고유한 파일명을 위한 파일명 리스트 추출\n",
    "    file_name_list = soup.find_all('span', class_='col m10 s10 x9 ctBx')\n",
    "    # tmp: 크롤링할 파일 url 목록 추출\n",
    "    tmp = soup.find_all('div', class_='fileGoupBox')\n",
    "    \n",
    "    # 2개의 리스트를 이용하기에 range 사용\n",
    "    for j in range(len(tmp)):\n",
    "        # 현재 file의 파일명 할당\n",
    "        file_name = file_name_list[j].find('span', class_='titlesub').getText().strip()\n",
    "        # 현재 file 위해 사용할 태그\n",
    "        for_download = tmp[j]\n",
    "        # naem_: 파일 확장자 확인 위한 변수\n",
    "        name_ = for_download.find('li').find('a').getText().strip()\n",
    "        if name_[-4:]=='.hwp':\n",
    "            url = ('http://www.bok.or.kr/'+for_download.find_all('li')[1].find('a')['href'])\n",
    "            with urllib.request.urlopen(url) as web_file, open(f'./{my_local_pdf_dir_path}/{file_name}.pdf', 'wb') as local_file:\n",
    "                local_file.write(web_file.read())\n",
    "        else:\n",
    "            url = ('http://www.bok.or.kr/'+for_download.find('li').find('a')['href'])\n",
    "            with urllib.request.urlopen(url) as web_file, open(f'./{my_local_pdf_dir_path}/{file_name}.pdf', 'wb') as local_file:\n",
    "                local_file.write(web_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "통과\n",
      "통과\n"
     ]
    }
   ],
   "source": [
    "# 채권 분석 리포트 https://finance.naver.com/research/debenture_list.naver\n",
    "import time\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "for i in range (1, 198):\n",
    "    bond_url = \"https://finance.naver.com/research/debenture_list.naver\"\n",
    "    params = {\n",
    "        'page' : i\n",
    "    }\n",
    "    headers = {'authority' : 'finance.naver.com',\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36' }\n",
    "   \n",
    "\n",
    "    response = requests.get(bond_url, params=params, headers=headers)\n",
    "    soup = BeautifulSoup(response.content)\n",
    "    bond = soup.select('#contentarea_left .file')\n",
    "    # bond_list_2 = soup.select('#contentarea_left .file')[0].find_previous_sibling('td').text\n",
    "    # bond_list= soup.select(\"#contentarea_left .file\")\n",
    "    # bond_list= soup.select(\"#contentarea_left .file a[href$='.pdf']\")\n",
    "    # print(bond_list[0].attrs['href'])\n",
    "    # print(bond_list_2)\n",
    "    # print(len(bond_list))\n",
    "    bond_list = []\n",
    "    for x in range(len(bond)):\n",
    "        # and bond[x].find_previous_sibling('td').find_previous_sibling('td').text[:2] == '채권'\n",
    "        if bond[x].find_previous_sibling('td').text == '유안타증권':\n",
    "            try:    \n",
    "                bond_list.append([soup.select(\"#contentarea_left .file a[href$='.pdf']\")[x].attrs['href'], bond[x].find_previous_sibling('td').find_previous_sibling('td').text])\n",
    "            except:\n",
    "                print(\"통과\")\n",
    "\n",
    "    for y in bond_list:\n",
    "        file_res = requests.get(y[0])\n",
    "        title = y[-1]\n",
    "        try:\n",
    "            with open('{}.pdf'.format(title), 'wb') as f:\n",
    "                f.write(file_res.content)\n",
    "        except:\n",
    "            print('통과')\n",
    "\n",
    "\n",
    "    \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "29865f05af80384af78d9233101596088e9d9771dcfe0a2d252f1a59aee54b06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
