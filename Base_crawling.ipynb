{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "# 의사록(MPB)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "for i in range(1, 45):\n",
    "    b_url = \"https://www.bok.or.kr/portal/bbs/B0000245/list.do?menuNo=200761\"\n",
    "    params = {\n",
    "        'pageIndex' : i\n",
    "    }\n",
    "\n",
    "    response = requests.get(b_url, params=params)\n",
    "    soup = BeautifulSoup(response.content)\n",
    "\n",
    "    li_list = []\n",
    "    li = soup.find('div', class_='bdLine type2').find('ul').find('li')\n",
    "    li_li = li.find_next_siblings('li', li)\n",
    "    li_list.append(li)\n",
    "    li_list.extend(li_li)\n",
    "\n",
    "    print(i)\n",
    "\n",
    "    for x in range(0, len(li_list)):\n",
    "        try:\n",
    "            link = li_list[x].find('div', class_='fileGoupBox').find('ul').find('li').find('a').attrs['href']\n",
    "            title = li_list[x].find('div', class_='row').find('span').find('a').find('span').find('span').text\n",
    "            url2 = 'http://www.bok.or.kr' + link\n",
    "            file_res = requests.get(url2)\n",
    "\n",
    "            with open('{}.hwp'.format(title), 'wb') as f:\n",
    "                f.write(file_res.content)\n",
    "        except:\n",
    "            link2 = li_list[x].find('div').find('div').find('a').attrs['href']\n",
    "            title = li_list[x].find('div', class_='row').find('span').find('a').find('span').find('span').text\n",
    "            url3 = 'http://www.bok.or.kr' + link2\n",
    "            file_res2 = requests.get(url3)\n",
    "\n",
    "            with open('{}.hwp'.format(title), 'wb') as f:\n",
    "                f.write(file_res.content)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "통과\n",
      "통과\n"
     ]
    }
   ],
   "source": [
    "# 채권 분석 리포트 https://finance.naver.com/research/debenture_list.naver\n",
    "import time\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "for i in range (1, 198):\n",
    "    bond_url = \"https://finance.naver.com/research/debenture_list.naver\"\n",
    "    params = {\n",
    "        'page' : i\n",
    "    }\n",
    "    headers = {'authority' : 'finance.naver.com',\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36' }\n",
    "   \n",
    "\n",
    "    response = requests.get(bond_url, params=params, headers=headers)\n",
    "    soup = BeautifulSoup(response.content)\n",
    "    bond = soup.select('#contentarea_left .file')\n",
    "    # bond_list_2 = soup.select('#contentarea_left .file')[0].find_previous_sibling('td').text\n",
    "    # bond_list= soup.select(\"#contentarea_left .file\")\n",
    "    # bond_list= soup.select(\"#contentarea_left .file a[href$='.pdf']\")\n",
    "    # print(bond_list[0].attrs['href'])\n",
    "    # print(bond_list_2)\n",
    "    # print(len(bond_list))\n",
    "    bond_list = []\n",
    "    for x in range(len(bond)):\n",
    "        # and bond[x].find_previous_sibling('td').find_previous_sibling('td').text[:2] == '채권'\n",
    "        if bond[x].find_previous_sibling('td').text == '유안타증권':\n",
    "            try:    \n",
    "                bond_list.append([soup.select(\"#contentarea_left .file a[href$='.pdf']\")[x].attrs['href'], bond[x].find_previous_sibling('td').find_previous_sibling('td').text])\n",
    "            except:\n",
    "                print(\"통과\")\n",
    "\n",
    "    for y in bond_list:\n",
    "        file_res = requests.get(y[0])\n",
    "        title = y[-1]\n",
    "        try:\n",
    "            with open('{}.pdf'.format(title), 'wb') as f:\n",
    "                f.write(file_res.content)\n",
    "        except:\n",
    "            print('통과')\n",
    "\n",
    "\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "29865f05af80384af78d9233101596088e9d9771dcfe0a2d252f1a59aee54b06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
