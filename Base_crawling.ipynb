{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "# 의사록(MPB)\n",
    "\n",
    "import requests, os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 각자 파일 경로\n",
    "my_local_pdf_dir_path = 'secret_MPB_minutes'\n",
    "# 디렉토리 생성\n",
    "os.makedirs(f'./{my_local_pdf_dir_path}', exist_ok=True)\n",
    "\n",
    "for i in range(1, 40):\n",
    "    b_url = \"https://www.bok.or.kr/portal/bbs/B0000245/list.do?menuNo=200761\"\n",
    "    params = {\n",
    "        'pageIndex' : i\n",
    "    }\n",
    "\n",
    "    response = requests.get(b_url, params=params)\n",
    "    soup = BeautifulSoup(response.content)\n",
    "\n",
    "    li_list = []\n",
    "    li = soup.find('div', class_='bdLine type2').find('ul').find('li')\n",
    "    li_li = li.find_next_siblings('li', li)\n",
    "    li_list.append(li)\n",
    "    li_list.extend(li_li)\n",
    "\n",
    "    print(i)\n",
    "\n",
    "    for x in range(0, len(li_list)):\n",
    "        try:\n",
    "            link = li_list[x].find('div', class_='fileGoupBox').find('ul').find('li').find('a').attrs['href']\n",
    "            name_ = li_list[x].find('div', class_='fileGoupBox').find('ul').find('li').find('a').getText().strip()\n",
    "            title = li_list[x].find('div', class_='row').find('span').find('a').find('span').find('span').text\n",
    "            url2 = 'http://www.bok.or.kr' + link\n",
    "            file_res = requests.get(url2)\n",
    "            if name_[-4:]=='.hwp':\n",
    "                link = li_list[x].find('div', class_='fileGoupBox').find('ul').find_all('li')[1].find('a').attrs['href']\n",
    "                url2 = 'http://www.bok.or.kr' + link\n",
    "                file_res = requests.get(url2)\n",
    "                with open('./{}/{}.pdf'.format(my_local_pdf_dir_path, title), 'wb') as f:\n",
    "                    f.write(file_res.content)\n",
    "            else:\n",
    "                with open('./{}/{}.pdf'.format(my_local_pdf_dir_path, title), 'wb') as f:\n",
    "                    f.write(file_res.content)\n",
    "\n",
    "        except:\n",
    "            try:\n",
    "                link2 = li_list[x].find('div').find('div').find('a').attrs['href']\n",
    "                name_ = li_list[x].find('div', class_='fileGoupBox').find('ul').find('li').find('a').getText().strip()\n",
    "                title = li_list[x].find('div', class_='row').find('span').find('a').find('span').find('span').text\n",
    "                url3 = 'http://www.bok.or.kr' + link2\n",
    "                file_res2 = requests.get(url3)\n",
    "                if name_[-4:]=='.hwp':\n",
    "                    link2 = li_list[x].find('div').find('div').find_all('a')[1].attrs['href']\n",
    "                    url3 = 'http://www.bok.or.kr' + link2\n",
    "                    with open('./{}/{}.pdf'.format(my_local_pdf_dir_path, title), 'wb') as f:\n",
    "                        f.write(file_res.content)\n",
    "                else:\n",
    "                    with open('./{}/{}.pdf'.format(my_local_pdf_dir_path, title), 'wb') as f:\n",
    "                        f.write(file_res.content)\n",
    "            except:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPB minutes pdf download code\n",
    "import requests, urllib, os\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# 각자 파일 경로\n",
    "my_local_pdf_dir_path = 'secret_MPB_minutes'\n",
    "# 디렉토리 생성\n",
    "os.makedirs(f'./{my_local_pdf_dir_path}', exist_ok=True)\n",
    "# b_url: base url, {page}: page number\n",
    "b_url = \"http://www.bok.or.kr/portal/bbs/B0000245/list.do?menuNo=200761&pageIndex={page}\"\n",
    "\n",
    "for i in tqdm(range(1, 40)):\n",
    "    response = requests.get(b_url.format(i))\n",
    "    soup = BeautifulSoup(response.content)\n",
    "\n",
    "    # file_name_list: 고유한 파일명을 위한 파일명 리스트 추출\n",
    "    file_name_list = soup.find_all('span', class_='col m10 s10 x9 ctBx')\n",
    "    # tmp: 크롤링할 파일 url 목록 추출\n",
    "    tmp = soup.find_all('div', class_='fileGoupBox')\n",
    "    \n",
    "    # 2개의 리스트를 이용하기에 range 사용\n",
    "    for j in range(len(tmp)):\n",
    "        # 현재 file의 파일명 할당\n",
    "        file_name = file_name_list[j].find('span', class_='titlesub').getText().strip()\n",
    "        # 현재 file 위해 사용할 태그\n",
    "        for_download = tmp[j]\n",
    "        # naem_: 파일 확장자 확인 위한 변수\n",
    "        name_ = for_download.find('li').find('a').getText().strip()\n",
    "        if name_[-4:]=='.hwp':\n",
    "            url = ('http://www.bok.or.kr/'+for_download.find_all('li')[1].find('a')['href'])\n",
    "            with urllib.request.urlopen(url) as web_file, open(f'./{my_local_pdf_dir_path}/{file_name}.pdf', 'wb') as local_file:\n",
    "                local_file.write(web_file.read())\n",
    "        else:\n",
    "            url = ('http://www.bok.or.kr/'+for_download.find('li').find('a')['href'])\n",
    "            with urllib.request.urlopen(url) as web_file, open(f'./{my_local_pdf_dir_path}/{file_name}.pdf', 'wb') as local_file:\n",
    "                local_file.write(web_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "통과\n",
      "통과\n"
     ]
    }
   ],
   "source": [
    "# 채권 분석 리포트 https://finance.naver.com/research/debenture_list.naver\n",
    "import time\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "for i in range (1, 198):\n",
    "    bond_url = \"https://finance.naver.com/research/debenture_list.naver\"\n",
    "    params = {\n",
    "        'page' : i\n",
    "    }\n",
    "    headers = {'authority' : 'finance.naver.com',\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36' }\n",
    "   \n",
    "\n",
    "    response = requests.get(bond_url, params=params, headers=headers)\n",
    "    soup = BeautifulSoup(response.content)\n",
    "    bond = soup.select('#contentarea_left .file')\n",
    "    bond_list = []\n",
    "    for x in range(len(bond)):\n",
    "        if bond[x].find_previous_sibling('td').text == '유안타증권':\n",
    "            try:    \n",
    "                bond_list.append([soup.select(\"#contentarea_left .file a[href$='.pdf']\")[x].attrs['href'], bond[x].find_previous_sibling('td').find_previous_sibling('td').text])\n",
    "            except:\n",
    "                print(\"통과\")\n",
    "\n",
    "    for y in bond_list:\n",
    "        file_res = requests.get(y[0])\n",
    "        title = y[-1]\n",
    "        try:\n",
    "            with open('{}.pdf'.format(title), 'wb') as f:\n",
    "                f.write(file_res.content)\n",
    "        except:\n",
    "            print('통과')\n",
    "\n",
    "\n",
    "    \n",
    "            "
   ]

  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 월별 기사수 '금리'라는 단어 포함\n",
    "2. 각 신문사별 기사수\n",
    "3. 채권분석데이터수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/130 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 39/43 [03:48<00:23,  5.86s/it]\n",
      "  0%|          | 0/130 [03:49<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(news_link):\n\u001b[0;32m     46\u001b[0m     news_resp \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(j, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m---> 47\u001b[0m     news_soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnews_resp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m         title \u001b[38;5;241m=\u001b[39m news_soup\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.article_info h3\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32mc:\\Users\\cyhgp\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:328\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    326\u001b[0m rejections \u001b[39m=\u001b[39m []\n\u001b[0;32m    327\u001b[0m success \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 328\u001b[0m \u001b[39mfor\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmarkup, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moriginal_encoding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeclared_html_encoding,\n\u001b[0;32m    329\u001b[0m  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontains_replacement_characters) \u001b[39min\u001b[39;00m (\n\u001b[0;32m    330\u001b[0m      \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mprepare_markup(\n\u001b[0;32m    331\u001b[0m          markup, from_encoding, exclude_encodings\u001b[39m=\u001b[39mexclude_encodings)):\n\u001b[0;32m    332\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset()\n\u001b[0;32m    333\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39minitialize_soup(\u001b[39mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\cyhgp\\anaconda3\\lib\\site-packages\\bs4\\builder\\_lxml.py:216\u001b[0m, in \u001b[0;36mLXMLTreeBuilderForXML.prepare_markup\u001b[1;34m(self, markup, user_specified_encoding, exclude_encodings, document_declared_encoding)\u001b[0m\n\u001b[0;32m    210\u001b[0m user_encodings \u001b[39m=\u001b[39m [document_declared_encoding]\n\u001b[0;32m    211\u001b[0m detector \u001b[39m=\u001b[39m EncodingDetector(\n\u001b[0;32m    212\u001b[0m     markup, known_definite_encodings\u001b[39m=\u001b[39mknown_definite_encodings,\n\u001b[0;32m    213\u001b[0m     user_encodings\u001b[39m=\u001b[39muser_encodings, is_html\u001b[39m=\u001b[39mis_html,\n\u001b[0;32m    214\u001b[0m     exclude_encodings\u001b[39m=\u001b[39mexclude_encodings\n\u001b[0;32m    215\u001b[0m )\n\u001b[1;32m--> 216\u001b[0m \u001b[39mfor\u001b[39;00m encoding \u001b[39min\u001b[39;00m detector\u001b[39m.\u001b[39mencodings:\n\u001b[0;32m    217\u001b[0m     \u001b[39myield\u001b[39;00m (detector\u001b[39m.\u001b[39mmarkup, encoding, document_declared_encoding, \u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\cyhgp\\anaconda3\\lib\\site-packages\\bs4\\dammit.py:442\u001b[0m, in \u001b[0;36mEncodingDetector.encodings\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39m# Use third-party character set detection to guess at the\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \u001b[39m# encoding.\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchardet_encoding \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 442\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchardet_encoding \u001b[39m=\u001b[39m chardet_dammit(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmarkup)\n\u001b[0;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_usable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchardet_encoding, tried):\n\u001b[0;32m    444\u001b[0m     \u001b[39myield\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchardet_encoding\n",
      "File \u001b[1;32mc:\\Users\\cyhgp\\anaconda3\\lib\\site-packages\\bs4\\dammit.py:46\u001b[0m, in \u001b[0;36mchardet_dammit\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(s, \u001b[39mstr\u001b[39m):\n\u001b[0;32m     45\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m \u001b[39mreturn\u001b[39;00m chardet_module\u001b[39m.\u001b[39;49mdetect(s)[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\cyhgp\\anaconda3\\lib\\site-packages\\chardet\\__init__.py:41\u001b[0m, in \u001b[0;36mdetect\u001b[1;34m(byte_str)\u001b[0m\n\u001b[0;32m     39\u001b[0m         byte_str \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(byte_str)\n\u001b[0;32m     40\u001b[0m detector \u001b[39m=\u001b[39m UniversalDetector()\n\u001b[1;32m---> 41\u001b[0m detector\u001b[39m.\u001b[39;49mfeed(byte_str)\n\u001b[0;32m     42\u001b[0m \u001b[39mreturn\u001b[39;00m detector\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\cyhgp\\anaconda3\\lib\\site-packages\\chardet\\universaldetector.py:211\u001b[0m, in \u001b[0;36mUniversalDetector.feed\u001b[1;34m(self, byte_str)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_charset_probers\u001b[39m.\u001b[39mappend(Latin1Prober())\n\u001b[0;32m    210\u001b[0m \u001b[39mfor\u001b[39;00m prober \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_charset_probers:\n\u001b[1;32m--> 211\u001b[0m     \u001b[39mif\u001b[39;00m prober\u001b[39m.\u001b[39;49mfeed(byte_str) \u001b[39m==\u001b[39m ProbingState\u001b[39m.\u001b[39mFOUND_IT:\n\u001b[0;32m    212\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresult \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m: prober\u001b[39m.\u001b[39mcharset_name,\n\u001b[0;32m    213\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mconfidence\u001b[39m\u001b[39m'\u001b[39m: prober\u001b[39m.\u001b[39mget_confidence(),\n\u001b[0;32m    214\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mlanguage\u001b[39m\u001b[39m'\u001b[39m: prober\u001b[39m.\u001b[39mlanguage}\n\u001b[0;32m    215\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cyhgp\\anaconda3\\lib\\site-packages\\chardet\\charsetgroupprober.py:71\u001b[0m, in \u001b[0;36mCharSetGroupProber.feed\u001b[1;34m(self, byte_str)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m prober\u001b[39m.\u001b[39mactive:\n\u001b[0;32m     70\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m state \u001b[39m=\u001b[39m prober\u001b[39m.\u001b[39;49mfeed(byte_str)\n\u001b[0;32m     72\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m state:\n\u001b[0;32m     73\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cyhgp\\anaconda3\\lib\\site-packages\\chardet\\mbcharsetprober.py:64\u001b[0m, in \u001b[0;36mMultiByteCharSetProber.feed\u001b[1;34m(self, byte_str)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(byte_str)):\n\u001b[0;32m     63\u001b[0m     coding_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoding_sm\u001b[39m.\u001b[39mnext_state(byte_str[i])\n\u001b[1;32m---> 64\u001b[0m     \u001b[39mif\u001b[39;00m coding_state \u001b[39m==\u001b[39;49m MachineState\u001b[39m.\u001b[39;49mERROR:\n\u001b[0;32m     65\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdebug(\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m prober hit error at byte \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[0;32m     66\u001b[0m                           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcharset_name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlanguage, i)\n\u001b[0;32m     67\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m=\u001b[39m ProbingState\u001b[39m.\u001b[39mNOT_ME\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# NNC(NaverNewsCrawling)\n",
    "import time\n",
    "import datetime\n",
    "import requests, os\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "date_date = pd.date_range('2012-12-30', '2023-08-31', freq='W') #2013년부터 2023년까지 뉴스데이터 크롤링\n",
    "date_role = [date_date[:130],date_date[130:260],date_date[260:390],date_date[390:520],date_date[520:]]\n",
    "\n",
    "dir_path = './secret_news_crawling'\n",
    "os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "for date in tqdm(date_role[0]):  #자신의 번호로 인덱싱\n",
    "    df = pd.DataFrame(columns =['신문사', '제목', '내용', '날짜'])\n",
    "    DateStart = date.strftime('%Y-%m-%d')\n",
    "    DateEnd = (date+datetime.timedelta(6)).strftime('%Y-%m-%d')\n",
    "    last_url = f\"https://finance.naver.com/news/news_search.naver?rcdate=&q=%B1%DD%B8%AE&x=20&y=14&sm=all.basic&pd=4&stDateStart={DateStart}&stDateEnd={DateEnd}&page=1\"\n",
    "    headers = {'authority' : 'finance.naver.com',\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'}\n",
    "    response = requests.get(last_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content)\n",
    "    last_page = soup.select('.pgRR a')[0]['href'].split('=')[-1] #마지막 페이지 가져오기\n",
    "    print(last_page)\n",
    "    for i in tqdm(range(1, int(last_page)+1)):   # 여기서 마지막 페이지까지 반복문\n",
    "        news_url = f\"https://finance.naver.com/news/news_search.naver?rcdate=&q=%B1%DD%B8%AE&x=20&y=14&sm=all.basic&pd=4&stDateStart={DateStart}&stDateEnd={DateEnd}&page={i}\"\n",
    "        headers = {'authority' : 'finance.naver.com',\n",
    "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'}\n",
    "        response = requests.get(news_url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content)\n",
    "        news = soup.select('.thumb a' )\n",
    "        magazine = soup.select('.articleSummary .press') #신문사 데이터\n",
    "        newsdate = soup.select('.articleSummary .wdate') # 날짜 데이터\n",
    "\n",
    "        news_link = []\n",
    "        magazine_name = []\n",
    "        dates = []\n",
    "        for i in range(len(news)):\n",
    "            news_link.append('https://finance.naver.com' + news[i].attrs['href'].replace('amp;', ''))  #amp;제거해서 링크활성화\n",
    "            magazine_name.append(magazine[i].text)\n",
    "            dates.append(newsdate[i])\n",
    "\n",
    "        total = []\n",
    "        for index, j in enumerate(news_link):\n",
    "            news_resp = requests.get(j, headers=headers)\n",
    "            news_soup = BeautifulSoup(news_resp.content)\n",
    "            try:\n",
    "                title = news_soup.select('.article_info h3')[0].text.strip()\n",
    "                content = news_soup.select('#content')[-1].text.strip()\n",
    "            except:\n",
    "                print(j)\n",
    "                continue\n",
    "            total.append([magazine_name[index],title, content, newsdate[index].text.lstrip()[:10]])\n",
    "        \n",
    "        df_temp = pd.DataFrame(total, columns = ['신문사', '제목', '내용', '날짜'])  #모든 리스트를 데이터프레임으로 전환\n",
    "        df = pd.concat([df, df_temp], ignore_index = True)\n",
    "    df.to_csv(f'{dir_path}/{DateStart}_{DateEnd}.csv', encoding='utf-8') # 1주일 간격으로 크롤링 데이터 저장    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/556 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'DatetimeIndex' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m last_page \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pgRR a\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# =기준 split으로 수정\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(last_page)\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlast_page\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m):   \u001b[38;5;66;03m# 여기서 다양한 최종페이지까지 설정\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     news_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://finance.naver.com/news/news_search.naver?rcdate=&q=%B1%DD%B8%AE&x=20&y=14&sm=all.basic&pd=4&stDateStart=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDateStart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&stDateEnd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDateEnd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&page=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m     headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthority\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinance.naver.com\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     28\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\u001b[39m\u001b[38;5;124m'\u001b[39m}\n",
      "\u001b[1;31mTypeError\u001b[0m: 'DatetimeIndex' object is not callable"
     ]
    }
   ],
   "source": [
    "# NNC(NaverNewsCrawling)\n",
    "import time\n",
    "import datetime\n",
    "import requests, os\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "date_range = pd.date_range('2013-01-01', '2023-08-31', freq='W')\n",
    "date_role = date_range[::111]\n",
    "date_range = pd.date_range('2013-01-01', '2023-08-31', freq='W')\n",
    "df = pd.DataFrame(columns =['신문사', '제목', '내용', '날짜'])\n",
    "\n",
    "\n",
    "for date in tqdm(date_range):\n",
    "    DateStart = date.strftime('%Y-%m-%d')\n",
    "    DateEnd = (date+datetime.timedelta(7)).strftime('%Y-%m-%d')\n",
    "    last_url = f\"https://finance.naver.com/news/news_search.naver?rcdate=&q=%B1%DD%B8%AE&x=20&y=14&sm=all.basic&pd=4&stDateStart={DateStart}&stDateEnd={DateEnd}&page=1\"\n",
    "    headers = {'authority' : 'finance.naver.com',\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'}\n",
    "    response = requests.get(last_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content)\n",
    "    last_page = soup.select('.pgRR a')[0]['href'].split('=')[-1] # =기준 split으로 수정\n",
    "    print(last_page)\n",
    "    for i in tqdm(range(1, int(last_page)+1)):   # 여기서 다양한 최종페이지까지 설정\n",
    "        news_url = f\"https://finance.naver.com/news/news_search.naver?rcdate=&q=%B1%DD%B8%AE&x=20&y=14&sm=all.basic&pd=4&stDateStart={DateStart}&stDateEnd={DateEnd}&page={i}\"\n",
    "        headers = {'authority' : 'finance.naver.com',\n",
    "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'}\n",
    "        response = requests.get(news_url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content)\n",
    "        news = soup.select('.thumb a' )\n",
    "        magazine = soup.select('.articleSummary .press')\n",
    "        date = soup.select('.articleSummary .wdate')\n",
    "\n",
    "        news_link = []\n",
    "        magazine_name = []\n",
    "        dates = []\n",
    "        for i in range(len(news)):\n",
    "            news_link.append('https://finance.naver.com' + news[i].attrs['href'].replace('amp;', ''))\n",
    "            magazine_name.append(magazine[i].text)\n",
    "            dates.append(date[i])\n",
    "\n",
    "        total = []\n",
    "        for index, j in enumerate(news_link):\n",
    "            news_resp = requests.get(j, headers=headers)\n",
    "            news_soup = BeautifulSoup(news_resp.content)\n",
    "            try:\n",
    "                title = news_soup.select('.article_info h3')[0].text.strip()\n",
    "                content = news_soup.select('#content')[-1].text.strip()\n",
    "            except:\n",
    "                print(j)\n",
    "                continue\n",
    "            total.append([magazine_name[index],title, content, date[index].text.lstrip()[:10]])\n",
    "        \n",
    "        df_temp = pd.DataFrame(total, columns = ['신문사', '제목', '내용', '날짜'])\n",
    "        df = pd.concat([df, df_temp], ignore_index = True)\n",
    "    df.to_csv(f'{DateStart}_{DateEnd}.csv', encoding='utf-8')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>신문사</th>\n",
       "      <th>제목</th>\n",
       "      <th>내용</th>\n",
       "      <th>날짜</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>세계일보</td>\n",
       "      <td>자영업자 신용대출 ‘저금리 대환’</td>\n",
       "      <td>코로나 기간내 고금리 대출자31일부터 최대 2000만원까지 신종 코로나바이러스 감염...</td>\n",
       "      <td>2023-08-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>경향신문</td>\n",
       "      <td>전세가구 이자비용 67% 급증… 월 20만원 첫 돌파</td>\n",
       "      <td>2023년 2분기 21만4000원 기록1년 반 만에 두 배 넘게 뛰어자가가구와 격차...</td>\n",
       "      <td>2023-08-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>경향신문</td>\n",
       "      <td>'잭슨홀 쇼크' 피했지만…'금리 딜레마' 여전한 한은</td>\n",
       "      <td>■우리 경제 여파는강도 낮췄지만 美 초긴축 여전유가상승 등으로 물가도 불안금리인하 ...</td>\n",
       "      <td>2023-08-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>세계일보</td>\n",
       "      <td>은행권 ‘부산 주 금고’ 잡기 ‘군침’… “지역 기여도 높여 평가해야”</td>\n",
       "      <td>지자체 등 70% 내년 계약 만료13곳 모두 부산은행 주 금고 거래시중은행 견제·도...</td>\n",
       "      <td>2023-08-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>서울경제</td>\n",
       "      <td>[단독] 슈퍼리치 놀이터라고?...10만원으로도 투자가능한 국채 나온다</td>\n",
       "      <td>서민 장기저축 지원 목적10년·20년 등 장기물로 출시만기보유시 분리과세·가산금리 ...</td>\n",
       "      <td>2023-08-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2609</th>\n",
       "      <td>2609</td>\n",
       "      <td>연합뉴스</td>\n",
       "      <td>중국발 경기 불안에…한은, 24일 기준금리 5연속 동결할 듯</td>\n",
       "      <td>전문가 \"2%p 한미 금리차·가계대출 증가에도 경기 우려로 못 올려\" \"한은, 올해...</td>\n",
       "      <td>2023-08-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2610</th>\n",
       "      <td>2610</td>\n",
       "      <td>연합뉴스</td>\n",
       "      <td>G2발 금융위기 확산…코스피 2,400까지 떨어지나</td>\n",
       "      <td>위안화·원화 약세에 외국인 매도압력…\"환율 1,300원대 후반 가능성\"\"중국 부동산...</td>\n",
       "      <td>2023-08-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2611</th>\n",
       "      <td>2611</td>\n",
       "      <td>YTN</td>\n",
       "      <td>문제 생기면 '은행 탓'…정부 가계빚 정책 엇박자 비판↑</td>\n",
       "      <td>금리 인하 옥죌 때는 언제고… 가계빚 늘자 또다시 은행권에 관리 주문50년 만기 주...</td>\n",
       "      <td>2023-08-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2612</th>\n",
       "      <td>2612</td>\n",
       "      <td>노컷뉴스</td>\n",
       "      <td>뉴욕증시, 엔비디아 실적 발표·연준 잭슨홀 컨퍼런스에 초점</td>\n",
       "      <td>[파이낸셜뉴스]   뉴욕증시 흐름을 좌우할 엔비디아 분기실적 발표가 23일(현지시간...</td>\n",
       "      <td>2023-08-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2613</th>\n",
       "      <td>2613</td>\n",
       "      <td>파이낸셜뉴스</td>\n",
       "      <td>비트코인, 이틀 만에 10% 이상 폭락</td>\n",
       "      <td>연합뉴스  추가 통화긴축 가능성이 위험자산 투자심리를 위축스페이스X 보유 비트코인 ...</td>\n",
       "      <td>2023-08-20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2614 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0      신문사                                       제목  \\\n",
       "0              0    세계일보                        자영업자 신용대출 ‘저금리 대환’   \n",
       "1              1    경향신문             전세가구 이자비용 67% 급증… 월 20만원 첫 돌파   \n",
       "2              2    경향신문             '잭슨홀 쇼크' 피했지만…'금리 딜레마' 여전한 한은   \n",
       "3              3    세계일보   은행권 ‘부산 주 금고’ 잡기 ‘군침’… “지역 기여도 높여 평가해야”   \n",
       "4              4    서울경제   [단독] 슈퍼리치 놀이터라고?...10만원으로도 투자가능한 국채 나온다   \n",
       "...          ...      ...                                      ...   \n",
       "2609        2609    연합뉴스         중국발 경기 불안에…한은, 24일 기준금리 5연속 동결할 듯   \n",
       "2610        2610    연합뉴스              G2발 금융위기 확산…코스피 2,400까지 떨어지나   \n",
       "2611        2611     YTN           문제 생기면 '은행 탓'…정부 가계빚 정책 엇박자 비판↑   \n",
       "2612        2612    노컷뉴스          뉴욕증시, 엔비디아 실적 발표·연준 잭슨홀 컨퍼런스에 초점   \n",
       "2613        2613  파이낸셜뉴스                     비트코인, 이틀 만에 10% 이상 폭락   \n",
       "\n",
       "                                                     내용          날짜  \n",
       "0     코로나 기간내 고금리 대출자31일부터 최대 2000만원까지 신종 코로나바이러스 감염...  2023-08-27  \n",
       "1     2023년 2분기 21만4000원 기록1년 반 만에 두 배 넘게 뛰어자가가구와 격차...  2023-08-27  \n",
       "2     ■우리 경제 여파는강도 낮췄지만 美 초긴축 여전유가상승 등으로 물가도 불안금리인하 ...  2023-08-27  \n",
       "3     지자체 등 70% 내년 계약 만료13곳 모두 부산은행 주 금고 거래시중은행 견제·도...  2023-08-27  \n",
       "4     서민 장기저축 지원 목적10년·20년 등 장기물로 출시만기보유시 분리과세·가산금리 ...  2023-08-27  \n",
       "...                                                 ...         ...  \n",
       "2609  전문가 \"2%p 한미 금리차·가계대출 증가에도 경기 우려로 못 올려\" \"한은, 올해...  2023-08-20  \n",
       "2610  위안화·원화 약세에 외국인 매도압력…\"환율 1,300원대 후반 가능성\"\"중국 부동산...  2023-08-20  \n",
       "2611  금리 인하 옥죌 때는 언제고… 가계빚 늘자 또다시 은행권에 관리 주문50년 만기 주...  2023-08-20  \n",
       "2612  [파이낸셜뉴스]   뉴욕증시 흐름을 좌우할 엔비디아 분기실적 발표가 23일(현지시간...  2023-08-20  \n",
       "2613  연합뉴스  추가 통화긴축 가능성이 위험자산 투자심리를 위축스페이스X 보유 비트코인 ...  2023-08-20  \n",
       "\n",
       "[2614 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "news = pd.read_csv('2023-08-20_2023-08-27.csv')\n",
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'141'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "today = datetime.datetime.today()\n",
    "\n",
    "DateStart = (today-datetime.timedelta(7)).strftime('%Y-%m-%d')\n",
    "DateEnd = today.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "last_url = f\"https://finance.naver.com/news/news_search.naver?rcdate=&q=%B1%DD%B8%AE&x=20&y=14&sm=all.basic&pd=4&stDateStart={DateStart}&stDateEnd={DateEnd}&page=1\"\n",
    "headers = {'authority' : 'finance.naver.com',\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'}\n",
    "response = requests.get(last_url, headers=headers)\n",
    "soup = BeautifulSoup(response.content)\n",
    "news = soup.select('.pgRR a')[0]['href'][-3:]\n",
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 채권 기사\n"
   ]

  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "29865f05af80384af78d9233101596088e9d9771dcfe0a2d252f1a59aee54b06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
